{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn9aNItt4okk"
   },
   "source": [
    "# Koç University, Deep Learning Course (COMP541) <br/> Assignment 4: Graph Convolution Networks\n",
    "\n",
    "In this assignment, you will implement the vanilla version of Graph Convolution\n",
    "Networks (GCN) [Kipf and Welling \\(2016\\)](https://arxiv.org/abs/1609.02907) and Graph Attention Networks (GAT) [Veličković, et al.\n",
    "\\(2018\\)](https://openreview.net/forum?id=rJXMpikCZ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ab_-ToD6EWN"
   },
   "source": [
    "## Background\n",
    "### Basics of GCN\n",
    "Recall from the lectures, the goal of a GCN is to learn a function of signals features on a graph $G = (V, E)$, which takes as inputs:\n",
    "1. he input features of each node, $x_i ∈ R^F$ (in matrix form: $X ∈ R^{|V |×F}$ )\n",
    "2. some information about the graph structure, typically the adjacency matrix $A$\n",
    "\n",
    "Each convolutional layer can be written as $H^{(l+1)} = f(H^{(l)}, A)$ for some function $f$. The function $f$ we are using for this assignment is in the form of $f(H^{(l)}, A) = σ(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)})$, where $\\hat{A} = A + I$ and $\\hat{D}$ is the diagonal node degree matrix ($D^{-1}\\hat{A}$ normalizes $\\hat{A}$ such that all rows sum to one). Let $\\tilde{A} = \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}$. The GCN we will implement takes two convolution layers, $Z = f(X, A) = softmax(\\tilde{A}~.~Dropout(ReLU(\\tilde{A}XW^{(0)}))~.W^{(1)})$\n",
    "\n",
    "### Basics of GAT\n",
    "Graph Attention Network (GAT) is a novel convolution-style neural network. It operates on graph-structured data and leverages masked self-attentional layers. In this assignment, we will implement the graph attention layer.\n",
    "\n",
    "### Dataset\n",
    "The dataset we used for this assignment is Cora ([Sen et al. \\(2008\\)](http://www.cs.iit.edu/~ml/pdfs/sen-aimag08.pdf)). Cora is one of standard citation network benchmark dataset (just like MNIST dataset for computer vision tasks). It that consists of 2708 scientific publications and 5429 links. Each publication is classified into one of 7 classes. Each publication is described by a word vector (length 1433) that indicates the absence/presence of the corresponding word. This is used as the features of each node for our experiment. The task is to perform node classification (predict which class each node belongs to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTsdgLCErn4Z"
   },
   "source": [
    "## Experiments\n",
    "Experiments:\n",
    "Open GCN notebook on Colab and implement the following parts.\n",
    "1. [1 pt] Implementation of Graph Convolution Layer: Complete the code for `GraphConvolution` Class\n",
    "2. [1 pt] Implementation of Graph Convolution Network: Complete the code for `GCN` Class\n",
    "3. [0.5 pt] Train your Graph Convolution Network: After implementing the required classes, now you can train your GCN. You can play with the hyperparameters in args.\n",
    "4. [2 pt] Implementation of Graph Attention Layer: Complete the code for `GraphAttentionLayer` Class\n",
    "5. [0.5 pt] Train your Graph Convolution Network: After implementing the required classes, now you can train your GAT. You can play with the hyperparameters in args.\n",
    "6. [0.5 pt] Compare your models: Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it’s the case (in 1-2 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImLCXm8IsSS2"
   },
   "source": [
    "# Download the Cora data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRN47p1SKRgP"
   },
   "outputs": [],
   "source": [
    "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "! tar -zxvf cora.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXIYzURA4OKg"
   },
   "source": [
    "# import modules and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uJQYMX02_z0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgOv1h7YsK-5"
   },
   "source": [
    "# Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kXPHN61i9keB"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./cora/\", dataset=\"cora\", training_samples=140):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj)\n",
    "\n",
    "    # Random indexes\n",
    "    idx_rand = torch.randperm(len(labels))\n",
    "    # Nodes for training\n",
    "    idx_train = idx_rand[:training_samples]\n",
    "    # Nodes for validation\n",
    "    idx_val= idx_rand[training_samples:]\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"symmetric normalization\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzCZVd1JsbHr"
   },
   "source": [
    "## check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KlsKjMKx8_b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mxrv21rLnpiZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "print(adj)\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lWrDf0iWnpqV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TUkt2JJdsuA2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "2708\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(labels.unique())\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iGP18jNAs1Gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "2568\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_train))\n",
    "print(len(idx_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHqIcfH-vIic"
   },
   "source": [
    "# Vanilla GCN for node classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48tylWyjLPE"
   },
   "source": [
    "## Define Graph Convolution layer (Your Task)\n",
    "\n",
    "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
    "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M-fU8L7f41VZ"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Convolution Layer (GCN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
    "        # hint: use nn.Linear()\n",
    "        ############ Your code here ###################################\n",
    "        self.W = torch.nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
    "        # to sum over neighbouring nodes )\n",
    "        # hint: use the linear layer you declared above. \n",
    "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
    "        #       adjacency matrix\n",
    "        ############ Your code here ###################################\n",
    "        Z = self.W(input) # --> equivalent to Z = X.B (Message Passing)\n",
    "        out = torch.spmm(adj.to_sparse(), Z) # --> equivalent to A(Z) (Aggregation)\n",
    "        return out\n",
    "\n",
    "        ###############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxBELCxkjF6F"
   },
   "source": [
    "## Define GCN (Your Task)\n",
    "\n",
    "You will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HtVr2cN8jD5t"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    A two-layer GCN\n",
    "    '''\n",
    "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
    "        \"\"\"\n",
    "        * `nfeat`, is the number of input features per node of the first layer\n",
    "        * `n_hidden`, number of hidden units\n",
    "        * `n_classes`, total number of classes for classification\n",
    "        * `dropout`, the dropout ratio\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "        # TODO: Initialization\n",
    "        # (1) 2 GraphConvolution() layers. \n",
    "        # (2) 1 Dropout layer\n",
    "        # (3) 1 activation function: ReLU()\n",
    "        ############ Your code here ###################################\n",
    "        self.graph_conv_1 = GraphConvolution(nfeat, n_hidden, bias)\n",
    "        self.graph_conv_2 = GraphConvolution(n_hidden, n_classes, bias)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        \n",
    "\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # TODO: the input will pass through the first graph convolution layer, \n",
    "        # the activation function, the dropout layer, then the second graph \n",
    "        # convolution layer. No activation function for the \n",
    "        # last layer. Return the logits. \n",
    "        ############ Your code here ###################################\n",
    "        out = self.graph_conv_1(x, adj)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.graph_conv_2(out, adj)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "        ###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX1d9F1G508r"
   },
   "source": [
    "## define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HyhqJ39OCzNN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXsdid6C5K1c"
   },
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bjlYeoFPFAWm"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qbx0uc-9G5vs"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TjNiui83FYBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W6tqqj16iz-"
   },
   "source": [
    "## training Vanilla GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WSjUYJPSlnOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9384 acc_train: 0.1143 loss_val: 1.9273 acc_val: 0.1597 time: 0.0427s\n",
      "Epoch: 0002 loss_train: 1.9314 acc_train: 0.1143 loss_val: 1.9222 acc_val: 0.1597 time: 0.0405s\n",
      "Epoch: 0003 loss_train: 1.9233 acc_train: 0.1143 loss_val: 1.9168 acc_val: 0.1608 time: 0.0392s\n",
      "Epoch: 0004 loss_train: 1.9164 acc_train: 0.1286 loss_val: 1.9105 acc_val: 0.2442 time: 0.0391s\n",
      "Epoch: 0005 loss_train: 1.9074 acc_train: 0.2214 loss_val: 1.9041 acc_val: 0.1367 time: 0.0391s\n",
      "Epoch: 0006 loss_train: 1.8962 acc_train: 0.1857 loss_val: 1.8974 acc_val: 0.1312 time: 0.0284s\n",
      "Epoch: 0007 loss_train: 1.8895 acc_train: 0.1643 loss_val: 1.8904 acc_val: 0.1873 time: 0.0301s\n",
      "Epoch: 0008 loss_train: 1.8753 acc_train: 0.3214 loss_val: 1.8828 acc_val: 0.3855 time: 0.0292s\n",
      "Epoch: 0009 loss_train: 1.8667 acc_train: 0.4000 loss_val: 1.8748 acc_val: 0.3372 time: 0.0277s\n",
      "Epoch: 0010 loss_train: 1.8555 acc_train: 0.3929 loss_val: 1.8665 acc_val: 0.3092 time: 0.0294s\n",
      "Epoch: 0011 loss_train: 1.8449 acc_train: 0.4071 loss_val: 1.8577 acc_val: 0.3018 time: 0.0287s\n",
      "Epoch: 0012 loss_train: 1.8356 acc_train: 0.3500 loss_val: 1.8487 acc_val: 0.3018 time: 0.0278s\n",
      "Epoch: 0013 loss_train: 1.8166 acc_train: 0.3429 loss_val: 1.8395 acc_val: 0.3014 time: 0.0278s\n",
      "Epoch: 0014 loss_train: 1.8072 acc_train: 0.3286 loss_val: 1.8303 acc_val: 0.3010 time: 0.0274s\n",
      "Epoch: 0015 loss_train: 1.7906 acc_train: 0.3357 loss_val: 1.8212 acc_val: 0.3010 time: 0.0305s\n",
      "Epoch: 0016 loss_train: 1.7662 acc_train: 0.3429 loss_val: 1.8122 acc_val: 0.3010 time: 0.0278s\n",
      "Epoch: 0017 loss_train: 1.7618 acc_train: 0.3286 loss_val: 1.8037 acc_val: 0.3010 time: 0.0274s\n",
      "Epoch: 0018 loss_train: 1.7389 acc_train: 0.3214 loss_val: 1.7957 acc_val: 0.3010 time: 0.0280s\n",
      "Epoch: 0019 loss_train: 1.7309 acc_train: 0.3214 loss_val: 1.7884 acc_val: 0.3010 time: 0.0294s\n",
      "Epoch: 0020 loss_train: 1.7323 acc_train: 0.3214 loss_val: 1.7820 acc_val: 0.3010 time: 0.0296s\n",
      "Epoch: 0021 loss_train: 1.7128 acc_train: 0.3214 loss_val: 1.7762 acc_val: 0.3010 time: 0.0293s\n",
      "Epoch: 0022 loss_train: 1.6965 acc_train: 0.3214 loss_val: 1.7709 acc_val: 0.3010 time: 0.0296s\n",
      "Epoch: 0023 loss_train: 1.6656 acc_train: 0.3214 loss_val: 1.7661 acc_val: 0.3010 time: 0.0335s\n",
      "Epoch: 0024 loss_train: 1.6794 acc_train: 0.3214 loss_val: 1.7613 acc_val: 0.3010 time: 0.0336s\n",
      "Epoch: 0025 loss_train: 1.6594 acc_train: 0.3214 loss_val: 1.7562 acc_val: 0.3010 time: 0.0339s\n",
      "Epoch: 0026 loss_train: 1.6486 acc_train: 0.3214 loss_val: 1.7506 acc_val: 0.3010 time: 0.0331s\n",
      "Epoch: 0027 loss_train: 1.6387 acc_train: 0.3214 loss_val: 1.7440 acc_val: 0.3010 time: 0.0338s\n",
      "Epoch: 0028 loss_train: 1.6228 acc_train: 0.3214 loss_val: 1.7364 acc_val: 0.3010 time: 0.0338s\n",
      "Epoch: 0029 loss_train: 1.6067 acc_train: 0.3286 loss_val: 1.7275 acc_val: 0.3014 time: 0.0346s\n",
      "Epoch: 0030 loss_train: 1.6027 acc_train: 0.3500 loss_val: 1.7178 acc_val: 0.3018 time: 0.0341s\n",
      "Epoch: 0031 loss_train: 1.5892 acc_train: 0.3429 loss_val: 1.7071 acc_val: 0.3018 time: 0.0341s\n",
      "Epoch: 0032 loss_train: 1.5582 acc_train: 0.3429 loss_val: 1.6958 acc_val: 0.3037 time: 0.0364s\n",
      "Epoch: 0033 loss_train: 1.5697 acc_train: 0.3714 loss_val: 1.6839 acc_val: 0.3072 time: 0.0338s\n",
      "Epoch: 0034 loss_train: 1.5292 acc_train: 0.3857 loss_val: 1.6718 acc_val: 0.3127 time: 0.0347s\n",
      "Epoch: 0035 loss_train: 1.5203 acc_train: 0.3929 loss_val: 1.6594 acc_val: 0.3213 time: 0.0346s\n",
      "Epoch: 0036 loss_train: 1.5035 acc_train: 0.4214 loss_val: 1.6466 acc_val: 0.3353 time: 0.0336s\n",
      "Epoch: 0037 loss_train: 1.4838 acc_train: 0.4571 loss_val: 1.6337 acc_val: 0.3516 time: 0.0371s\n",
      "Epoch: 0038 loss_train: 1.4702 acc_train: 0.4929 loss_val: 1.6208 acc_val: 0.3711 time: 0.0349s\n",
      "Epoch: 0039 loss_train: 1.4505 acc_train: 0.4643 loss_val: 1.6079 acc_val: 0.3855 time: 0.0334s\n",
      "Epoch: 0040 loss_train: 1.4443 acc_train: 0.5000 loss_val: 1.5951 acc_val: 0.3956 time: 0.0340s\n",
      "Epoch: 0041 loss_train: 1.4241 acc_train: 0.5286 loss_val: 1.5824 acc_val: 0.4058 time: 0.0345s\n",
      "Epoch: 0042 loss_train: 1.4216 acc_train: 0.5500 loss_val: 1.5697 acc_val: 0.4132 time: 0.0348s\n",
      "Epoch: 0043 loss_train: 1.3945 acc_train: 0.5500 loss_val: 1.5574 acc_val: 0.4202 time: 0.0335s\n",
      "Epoch: 0044 loss_train: 1.3846 acc_train: 0.5429 loss_val: 1.5457 acc_val: 0.4280 time: 0.0334s\n",
      "Epoch: 0045 loss_train: 1.3642 acc_train: 0.5571 loss_val: 1.5344 acc_val: 0.4357 time: 0.0332s\n",
      "Epoch: 0046 loss_train: 1.3197 acc_train: 0.5857 loss_val: 1.5236 acc_val: 0.4396 time: 0.0319s\n",
      "Epoch: 0047 loss_train: 1.3219 acc_train: 0.5714 loss_val: 1.5131 acc_val: 0.4478 time: 0.0315s\n",
      "Epoch: 0048 loss_train: 1.3321 acc_train: 0.5643 loss_val: 1.5029 acc_val: 0.4525 time: 0.0281s\n",
      "Epoch: 0049 loss_train: 1.2971 acc_train: 0.6071 loss_val: 1.4924 acc_val: 0.4599 time: 0.0283s\n",
      "Epoch: 0050 loss_train: 1.2812 acc_train: 0.5643 loss_val: 1.4818 acc_val: 0.4692 time: 0.0279s\n",
      "Epoch: 0051 loss_train: 1.2815 acc_train: 0.5929 loss_val: 1.4712 acc_val: 0.4751 time: 0.0277s\n",
      "Epoch: 0052 loss_train: 1.2470 acc_train: 0.5643 loss_val: 1.4606 acc_val: 0.4825 time: 0.0277s\n",
      "Epoch: 0053 loss_train: 1.2456 acc_train: 0.6143 loss_val: 1.4502 acc_val: 0.4910 time: 0.0278s\n",
      "Epoch: 0054 loss_train: 1.2033 acc_train: 0.6000 loss_val: 1.4393 acc_val: 0.4992 time: 0.0308s\n",
      "Epoch: 0055 loss_train: 1.2047 acc_train: 0.6500 loss_val: 1.4280 acc_val: 0.5082 time: 0.0278s\n",
      "Epoch: 0056 loss_train: 1.1921 acc_train: 0.6214 loss_val: 1.4163 acc_val: 0.5125 time: 0.0276s\n",
      "Epoch: 0057 loss_train: 1.1964 acc_train: 0.6429 loss_val: 1.4048 acc_val: 0.5206 time: 0.0277s\n",
      "Epoch: 0058 loss_train: 1.1480 acc_train: 0.6643 loss_val: 1.3936 acc_val: 0.5249 time: 0.0276s\n",
      "Epoch: 0059 loss_train: 1.1623 acc_train: 0.6214 loss_val: 1.3827 acc_val: 0.5273 time: 0.0275s\n",
      "Epoch: 0060 loss_train: 1.1130 acc_train: 0.6643 loss_val: 1.3725 acc_val: 0.5296 time: 0.0276s\n",
      "Epoch: 0061 loss_train: 1.1219 acc_train: 0.6429 loss_val: 1.3623 acc_val: 0.5327 time: 0.0278s\n",
      "Epoch: 0062 loss_train: 1.0890 acc_train: 0.6357 loss_val: 1.3519 acc_val: 0.5405 time: 0.0313s\n",
      "Epoch: 0063 loss_train: 1.1070 acc_train: 0.6357 loss_val: 1.3415 acc_val: 0.5471 time: 0.0281s\n",
      "Epoch: 0064 loss_train: 1.0635 acc_train: 0.6500 loss_val: 1.3316 acc_val: 0.5553 time: 0.0277s\n",
      "Epoch: 0065 loss_train: 1.0614 acc_train: 0.6929 loss_val: 1.3225 acc_val: 0.5631 time: 0.0278s\n",
      "Epoch: 0066 loss_train: 1.0727 acc_train: 0.6643 loss_val: 1.3143 acc_val: 0.5678 time: 0.0278s\n",
      "Epoch: 0067 loss_train: 1.0420 acc_train: 0.6786 loss_val: 1.3062 acc_val: 0.5740 time: 0.0291s\n",
      "Epoch: 0068 loss_train: 1.0316 acc_train: 0.7143 loss_val: 1.2981 acc_val: 0.5787 time: 0.0295s\n",
      "Epoch: 0069 loss_train: 1.0082 acc_train: 0.6786 loss_val: 1.2893 acc_val: 0.5790 time: 0.0293s\n",
      "Epoch: 0070 loss_train: 0.9927 acc_train: 0.7214 loss_val: 1.2800 acc_val: 0.5771 time: 0.0391s\n",
      "Epoch: 0071 loss_train: 0.9929 acc_train: 0.6929 loss_val: 1.2705 acc_val: 0.5759 time: 0.0321s\n",
      "Epoch: 0072 loss_train: 0.9752 acc_train: 0.7000 loss_val: 1.2607 acc_val: 0.5771 time: 0.0326s\n",
      "Epoch: 0073 loss_train: 0.9886 acc_train: 0.7000 loss_val: 1.2512 acc_val: 0.5775 time: 0.0342s\n",
      "Epoch: 0074 loss_train: 0.9616 acc_train: 0.7214 loss_val: 1.2422 acc_val: 0.5783 time: 0.0353s\n",
      "Epoch: 0075 loss_train: 0.9206 acc_train: 0.7429 loss_val: 1.2338 acc_val: 0.5775 time: 0.0352s\n",
      "Epoch: 0076 loss_train: 0.9545 acc_train: 0.7214 loss_val: 1.2255 acc_val: 0.5798 time: 0.0343s\n",
      "Epoch: 0077 loss_train: 0.8999 acc_train: 0.7357 loss_val: 1.2168 acc_val: 0.5864 time: 0.0354s\n",
      "Epoch: 0078 loss_train: 0.8949 acc_train: 0.7714 loss_val: 1.2084 acc_val: 0.5923 time: 0.0332s\n",
      "Epoch: 0079 loss_train: 0.9337 acc_train: 0.7143 loss_val: 1.1994 acc_val: 0.5989 time: 0.0334s\n",
      "Epoch: 0080 loss_train: 0.9046 acc_train: 0.7429 loss_val: 1.1907 acc_val: 0.6028 time: 0.0332s\n",
      "Epoch: 0081 loss_train: 0.8666 acc_train: 0.7929 loss_val: 1.1825 acc_val: 0.6090 time: 0.0319s\n",
      "Epoch: 0082 loss_train: 0.8861 acc_train: 0.7643 loss_val: 1.1747 acc_val: 0.6153 time: 0.0294s\n",
      "Epoch: 0083 loss_train: 0.8510 acc_train: 0.8000 loss_val: 1.1674 acc_val: 0.6180 time: 0.0330s\n",
      "Epoch: 0084 loss_train: 0.8702 acc_train: 0.8143 loss_val: 1.1600 acc_val: 0.6223 time: 0.0309s\n",
      "Epoch: 0085 loss_train: 0.8494 acc_train: 0.8071 loss_val: 1.1525 acc_val: 0.6234 time: 0.0317s\n",
      "Epoch: 0086 loss_train: 0.8179 acc_train: 0.8071 loss_val: 1.1458 acc_val: 0.6215 time: 0.0357s\n",
      "Epoch: 0087 loss_train: 0.8139 acc_train: 0.7857 loss_val: 1.1389 acc_val: 0.6231 time: 0.0338s\n",
      "Epoch: 0088 loss_train: 0.8124 acc_train: 0.7929 loss_val: 1.1319 acc_val: 0.6258 time: 0.0345s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 loss_train: 0.8042 acc_train: 0.8000 loss_val: 1.1247 acc_val: 0.6262 time: 0.0344s\n",
      "Epoch: 0090 loss_train: 0.8213 acc_train: 0.7500 loss_val: 1.1180 acc_val: 0.6250 time: 0.0353s\n",
      "Epoch: 0091 loss_train: 0.7789 acc_train: 0.8429 loss_val: 1.1107 acc_val: 0.6269 time: 0.0358s\n",
      "Epoch: 0092 loss_train: 0.7801 acc_train: 0.8143 loss_val: 1.1029 acc_val: 0.6324 time: 0.0335s\n",
      "Epoch: 0093 loss_train: 0.7579 acc_train: 0.8286 loss_val: 1.0952 acc_val: 0.6394 time: 0.0336s\n",
      "Epoch: 0094 loss_train: 0.7888 acc_train: 0.8143 loss_val: 1.0873 acc_val: 0.6456 time: 0.0334s\n",
      "Epoch: 0095 loss_train: 0.7326 acc_train: 0.8571 loss_val: 1.0805 acc_val: 0.6503 time: 0.0338s\n",
      "Epoch: 0096 loss_train: 0.7428 acc_train: 0.8357 loss_val: 1.0742 acc_val: 0.6573 time: 0.0346s\n",
      "Epoch: 0097 loss_train: 0.7481 acc_train: 0.8643 loss_val: 1.0683 acc_val: 0.6604 time: 0.0334s\n",
      "Epoch: 0098 loss_train: 0.7307 acc_train: 0.8571 loss_val: 1.0629 acc_val: 0.6612 time: 0.0317s\n",
      "Epoch: 0099 loss_train: 0.7323 acc_train: 0.8571 loss_val: 1.0574 acc_val: 0.6628 time: 0.0294s\n",
      "Epoch: 0100 loss_train: 0.7306 acc_train: 0.8429 loss_val: 1.0518 acc_val: 0.6643 time: 0.0344s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 3.2126s\n",
      "Test set results: loss= 1.0518 accuracy= 0.6643\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# evaluating\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKHEyXp1EVdo"
   },
   "source": [
    "# Graph Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx15HdotKnt_"
   },
   "source": [
    "## Graph attention layer (Your task)\n",
    "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
    "\n",
    "\n",
    "### The initial transformation\n",
    "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
    "\n",
    "### attention score\n",
    "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
    "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
    "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
    "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
    "\n",
    "#### How to vectorize this? Some hints: \n",
    "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
    "\n",
    "2. `tensor.repeat_interleave()` gives you\n",
    "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
    "\n",
    "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
    "\n",
    "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
    "\n",
    "\n",
    "#### Perform softmax \n",
    "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
    "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "\n",
    "#### Apply dropout\n",
    "Apply the dropout layer. (this step is easy)\n",
    "\n",
    "#### Calculate final output for each head\n",
    "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
    "\n",
    "\n",
    "#### Concat or Mean\n",
    "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wVu7rcOuAUZz"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 alpha: float = 0.2):\n",
    "        \"\"\"\n",
    "        in_features: F, the number of input features per node\n",
    "        out_features: F', the number of output features per node\n",
    "        n_heads: K, the number of attention heads\n",
    "        is_concat: whether the multi-head results should be concatenated or averaged\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # TODO: initialize the following modules: \n",
    "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
    "        # You should NOT use for loops for the multiheaded implementation (set bias = False)\n",
    "        # (2) self.attention: Linear layer that compute the attention score (set bias = False)\n",
    "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
    "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
    "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
    "        ################ your code here ########################\n",
    "        self.W = torch.nn.Linear(in_features, (self.n_hidden * self.n_heads), bias=False)\n",
    "        self.attention = torch.nn.Linear((2 * self.n_hidden), 1, bias=False)\n",
    "        self.activation = torch.nn.LeakyReLU(negative_slope=alpha)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        ########################################################\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        \n",
    "        # TODO: \n",
    "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
    "        #     (you can use tensor.view() function)\n",
    "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
    "        # (3) apply the attention layer \n",
    "        # (4) apply the activation layer (you will get the attention score e)\n",
    "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
    "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
    "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
    "        # (7) apply softmax \n",
    "        # (8) apply dropout_layer \n",
    "        ############## Your code here #########################################\n",
    "        # (1)\n",
    "        s = self.W(h) # --> [n_nodes, in_features] * [in_features, n_heads * n_hidden] = [n_nodes, n_heads * n_hidden]\n",
    "        # s reshape to separate features for different attention heads\n",
    "        #s = torch.reshape(s, (n_nodes, self.n_heads, -1)) # --> [n_nodes, n_heads, n_hidden]\n",
    "        s = s.view(n_nodes, self.n_heads, -1) # --> [n_nodes, n_heads, n_hidden]\n",
    "        # (2)\n",
    "        s_repeated = s.repeat(n_nodes, 1, 1) # --> [n_nodes * n_nodes, n_heads, n_hidden]\n",
    "        s_interleaved = torch.repeat_interleave(s, n_nodes, dim=0) # --> [n_nodes * n_nodes, n_heads, n_hidden]\n",
    "        # Note that they are like follows\n",
    "        # s_repeated = [s1, s2, ..., sn, s1, s2, ..., sn]\n",
    "        # s_interleaved = [s1, s1, ..., s1, s2, s2, ..., s2, sn, sn ..., sn]\n",
    "        s_concatenated = torch.cat((s_interleaved, s_repeated), dim=-1) # --> [n_nodes * n_nodes, n_heads, 2 * n_hidden]\n",
    "        #s_concatenated = torch.reshape(s_concatenated, (n_nodes, n_nodes, self.n_heads, -1)) # --> [n_nodes, n_nodes, n_heads, 2 * n_hidden]\n",
    "        s_concatenated = s_concatenated.view(n_nodes, n_nodes, self.n_heads, -1) # --> [n_nodes, n_nodes, n_heads, 2 * n_hidden]\n",
    "        \n",
    "        \n",
    "        # (3)\n",
    "        e = self.attention(s_concatenated) # --> [n_nodes, n_nodes, n_heads, 1]\n",
    "        \n",
    "        # (4)\n",
    "        e = self.activation(e) # --> [n_nodes, n_nodes, n_heads, 1]\n",
    "        \n",
    "        # (5)\n",
    "        e = torch.squeeze(e, dim=-1) # --> [n_nodes, n_nodes, n_heads]\n",
    "        \n",
    "        # (6)\n",
    "        # Note that adj_mat --> [n_nodes, n_nodes]\n",
    "        adj_mat = torch.unsqueeze(adj_mat, dim=-1) # --> [n_nodes, n_nodes, 1]\n",
    "        e = e.masked_fill(adj_mat == 0, -float(\"inf\")) # --> [n_nodes, n_nodes, n_heads]\n",
    "        \n",
    "        # (7)\n",
    "        a = self.softmax(e)\n",
    "        \n",
    "        # (8)\n",
    "        a = self.dropout(a) # --> [n_nodes, n_nodes, n_heads]\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        # Summation \n",
    "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
    "\n",
    "\n",
    "        # TODO: Concat or Mean\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            ############## Your code here #########################################\n",
    "            #out =  torch.reshape(h_prime, (n_nodes, -1))# --> [n_nodes, n_heads * n_hidden]\n",
    "            out =  h_prime.contiguous().view(n_nodes, -1)# --> [n_nodes, n_heads * n_hidden]\n",
    "            return out\n",
    "            #######################################################################\n",
    "        # Take the mean of the heads (for the last layer)\n",
    "        else:\n",
    "            ############## Your code here #########################################\n",
    "            out = torch.mean(h_prime, dim=1)\n",
    "            return out\n",
    "\n",
    "            #######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOSk_ZShi2nR"
   },
   "source": [
    "## Define GAT network\n",
    "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jKNbUtPVi1Vs"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
    "        \"\"\"\n",
    "        in_features: the number of features per node\n",
    "        n_hidden: the number of features in the first graph attention layer\n",
    "        n_classes: the number of classes\n",
    "        n_heads: the number of heads in the graph attention layers\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First graph attention layer where we concatenate the heads\n",
    "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
    "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
    "        self.activation = nn.ELU()  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: the features vectors\n",
    "        adj_mat: the adjacency matrix\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc1(x, adj_mat)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj_mat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtRQ3Ced7RAw"
   },
   "source": [
    "## training GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "b7D5mYXC6zTG"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        \"alpha\": 0.2,\n",
    "        \"n_heads\": 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7MYaK98hDy7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GAT(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"],\n",
    "            alpha=args[\"alpha\"],\n",
    "            n_heads=args[\"n_heads\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "E9FcfXwMDzEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9460 acc_train: 0.1143 loss_val: 1.9429 acc_val: 0.4015 time: 5.0306s\n",
      "Epoch: 0002 loss_train: 1.9417 acc_train: 0.4857 loss_val: 1.9396 acc_val: 0.4642 time: 5.1252s\n",
      "Epoch: 0003 loss_train: 1.9371 acc_train: 0.5214 loss_val: 1.9360 acc_val: 0.4638 time: 5.1245s\n",
      "Epoch: 0004 loss_train: 1.9319 acc_train: 0.5143 loss_val: 1.9319 acc_val: 0.4607 time: 5.1362s\n",
      "Epoch: 0005 loss_train: 1.9260 acc_train: 0.4857 loss_val: 1.9275 acc_val: 0.4556 time: 5.1593s\n",
      "Epoch: 0006 loss_train: 1.9223 acc_train: 0.5643 loss_val: 1.9227 acc_val: 0.4587 time: 5.1515s\n",
      "Epoch: 0007 loss_train: 1.9153 acc_train: 0.5214 loss_val: 1.9175 acc_val: 0.4583 time: 5.1577s\n",
      "Epoch: 0008 loss_train: 1.9049 acc_train: 0.5214 loss_val: 1.9119 acc_val: 0.4587 time: 5.1772s\n",
      "Epoch: 0009 loss_train: 1.8980 acc_train: 0.5286 loss_val: 1.9059 acc_val: 0.4599 time: 5.1682s\n",
      "Epoch: 0010 loss_train: 1.8881 acc_train: 0.4857 loss_val: 1.8994 acc_val: 0.4603 time: 5.1660s\n",
      "Epoch: 0011 loss_train: 1.8770 acc_train: 0.5214 loss_val: 1.8926 acc_val: 0.4595 time: 5.2488s\n",
      "Epoch: 0012 loss_train: 1.8717 acc_train: 0.5000 loss_val: 1.8853 acc_val: 0.4579 time: 5.1842s\n",
      "Epoch: 0013 loss_train: 1.8530 acc_train: 0.5357 loss_val: 1.8776 acc_val: 0.4548 time: 5.2373s\n",
      "Epoch: 0014 loss_train: 1.8442 acc_train: 0.5000 loss_val: 1.8694 acc_val: 0.4548 time: 5.2894s\n",
      "Epoch: 0015 loss_train: 1.8404 acc_train: 0.4929 loss_val: 1.8609 acc_val: 0.4540 time: 5.2670s\n",
      "Epoch: 0016 loss_train: 1.8322 acc_train: 0.5357 loss_val: 1.8519 acc_val: 0.4521 time: 5.3286s\n",
      "Epoch: 0017 loss_train: 1.8258 acc_train: 0.5500 loss_val: 1.8428 acc_val: 0.4505 time: 5.2607s\n",
      "Epoch: 0018 loss_train: 1.8025 acc_train: 0.4786 loss_val: 1.8333 acc_val: 0.4498 time: 5.4387s\n",
      "Epoch: 0019 loss_train: 1.7842 acc_train: 0.4857 loss_val: 1.8236 acc_val: 0.4482 time: 5.3873s\n",
      "Epoch: 0020 loss_train: 1.7739 acc_train: 0.4786 loss_val: 1.8135 acc_val: 0.4486 time: 5.3808s\n",
      "Epoch: 0021 loss_train: 1.7551 acc_train: 0.4929 loss_val: 1.8033 acc_val: 0.4482 time: 5.3102s\n",
      "Epoch: 0022 loss_train: 1.7502 acc_train: 0.4857 loss_val: 1.7930 acc_val: 0.4482 time: 5.3014s\n",
      "Epoch: 0023 loss_train: 1.7088 acc_train: 0.5286 loss_val: 1.7823 acc_val: 0.4486 time: 5.2527s\n",
      "Epoch: 0024 loss_train: 1.7159 acc_train: 0.4786 loss_val: 1.7716 acc_val: 0.4490 time: 5.3152s\n",
      "Epoch: 0025 loss_train: 1.6973 acc_train: 0.5214 loss_val: 1.7606 acc_val: 0.4486 time: 5.2266s\n",
      "Epoch: 0026 loss_train: 1.6792 acc_train: 0.5143 loss_val: 1.7494 acc_val: 0.4482 time: 5.2442s\n",
      "Epoch: 0027 loss_train: 1.6942 acc_train: 0.4357 loss_val: 1.7383 acc_val: 0.4486 time: 5.2057s\n",
      "Epoch: 0028 loss_train: 1.6552 acc_train: 0.4857 loss_val: 1.7271 acc_val: 0.4486 time: 5.1994s\n",
      "Epoch: 0029 loss_train: 1.6367 acc_train: 0.5000 loss_val: 1.7159 acc_val: 0.4478 time: 5.2898s\n",
      "Epoch: 0030 loss_train: 1.6265 acc_train: 0.5143 loss_val: 1.7045 acc_val: 0.4490 time: 5.3292s\n",
      "Epoch: 0031 loss_train: 1.5881 acc_train: 0.4929 loss_val: 1.6932 acc_val: 0.4502 time: 5.3719s\n",
      "Epoch: 0032 loss_train: 1.5460 acc_train: 0.5143 loss_val: 1.6818 acc_val: 0.4533 time: 5.3453s\n",
      "Epoch: 0033 loss_train: 1.6333 acc_train: 0.5286 loss_val: 1.6705 acc_val: 0.4568 time: 5.2705s\n",
      "Epoch: 0034 loss_train: 1.5799 acc_train: 0.5286 loss_val: 1.6593 acc_val: 0.4603 time: 5.2818s\n",
      "Epoch: 0035 loss_train: 1.5327 acc_train: 0.5571 loss_val: 1.6480 acc_val: 0.4630 time: 5.2748s\n",
      "Epoch: 0036 loss_train: 1.5704 acc_train: 0.5214 loss_val: 1.6368 acc_val: 0.4704 time: 5.2576s\n",
      "Epoch: 0037 loss_train: 1.5215 acc_train: 0.5357 loss_val: 1.6254 acc_val: 0.4759 time: 5.3101s\n",
      "Epoch: 0038 loss_train: 1.4864 acc_train: 0.5357 loss_val: 1.6140 acc_val: 0.4875 time: 5.2499s\n",
      "Epoch: 0039 loss_train: 1.4717 acc_train: 0.5571 loss_val: 1.6024 acc_val: 0.5023 time: 5.2136s\n",
      "Epoch: 0040 loss_train: 1.4941 acc_train: 0.5857 loss_val: 1.5910 acc_val: 0.5175 time: 5.2182s\n",
      "Epoch: 0041 loss_train: 1.3876 acc_train: 0.6000 loss_val: 1.5794 acc_val: 0.5331 time: 5.2680s\n",
      "Epoch: 0042 loss_train: 1.4662 acc_train: 0.5929 loss_val: 1.5679 acc_val: 0.5436 time: 5.2221s\n",
      "Epoch: 0043 loss_train: 1.4985 acc_train: 0.5857 loss_val: 1.5565 acc_val: 0.5584 time: 5.2301s\n",
      "Epoch: 0044 loss_train: 1.4341 acc_train: 0.5786 loss_val: 1.5452 acc_val: 0.5713 time: 5.2054s\n",
      "Epoch: 0045 loss_train: 1.4258 acc_train: 0.6143 loss_val: 1.5339 acc_val: 0.5857 time: 5.2470s\n",
      "Epoch: 0046 loss_train: 1.3756 acc_train: 0.5929 loss_val: 1.5225 acc_val: 0.5931 time: 5.2703s\n",
      "Epoch: 0047 loss_train: 1.3404 acc_train: 0.6500 loss_val: 1.5109 acc_val: 0.5989 time: 5.2913s\n",
      "Epoch: 0048 loss_train: 1.3884 acc_train: 0.6857 loss_val: 1.4994 acc_val: 0.6059 time: 5.2699s\n",
      "Epoch: 0049 loss_train: 1.3857 acc_train: 0.6571 loss_val: 1.4881 acc_val: 0.6133 time: 5.3193s\n",
      "Epoch: 0050 loss_train: 1.3165 acc_train: 0.6714 loss_val: 1.4769 acc_val: 0.6168 time: 5.3393s\n",
      "Epoch: 0051 loss_train: 1.3737 acc_train: 0.6500 loss_val: 1.4658 acc_val: 0.6234 time: 5.2827s\n",
      "Epoch: 0052 loss_train: 1.3076 acc_train: 0.7071 loss_val: 1.4549 acc_val: 0.6320 time: 5.2317s\n",
      "Epoch: 0053 loss_train: 1.3014 acc_train: 0.6929 loss_val: 1.4440 acc_val: 0.6406 time: 5.3877s\n",
      "Epoch: 0054 loss_train: 1.3008 acc_train: 0.7214 loss_val: 1.4332 acc_val: 0.6452 time: 5.2416s\n",
      "Epoch: 0055 loss_train: 1.2997 acc_train: 0.6857 loss_val: 1.4225 acc_val: 0.6526 time: 5.3139s\n",
      "Epoch: 0056 loss_train: 1.2612 acc_train: 0.7429 loss_val: 1.4119 acc_val: 0.6600 time: 5.2952s\n",
      "Epoch: 0057 loss_train: 1.2380 acc_train: 0.6714 loss_val: 1.4014 acc_val: 0.6600 time: 5.3043s\n",
      "Epoch: 0058 loss_train: 1.2418 acc_train: 0.7357 loss_val: 1.3908 acc_val: 0.6674 time: 5.2574s\n",
      "Epoch: 0059 loss_train: 1.2681 acc_train: 0.7143 loss_val: 1.3803 acc_val: 0.6733 time: 5.3532s\n",
      "Epoch: 0060 loss_train: 1.2113 acc_train: 0.7143 loss_val: 1.3699 acc_val: 0.6811 time: 5.3139s\n",
      "Epoch: 0061 loss_train: 1.1940 acc_train: 0.7786 loss_val: 1.3594 acc_val: 0.6869 time: 5.2660s\n",
      "Epoch: 0062 loss_train: 1.2825 acc_train: 0.7143 loss_val: 1.3492 acc_val: 0.6904 time: 5.1978s\n",
      "Epoch: 0063 loss_train: 1.2626 acc_train: 0.7071 loss_val: 1.3393 acc_val: 0.6947 time: 5.2831s\n",
      "Epoch: 0064 loss_train: 1.1465 acc_train: 0.7357 loss_val: 1.3292 acc_val: 0.6994 time: 5.1888s\n",
      "Epoch: 0065 loss_train: 1.1426 acc_train: 0.7643 loss_val: 1.3193 acc_val: 0.7033 time: 5.2993s\n",
      "Epoch: 0066 loss_train: 1.2073 acc_train: 0.7357 loss_val: 1.3096 acc_val: 0.7068 time: 5.4408s\n",
      "Epoch: 0067 loss_train: 1.1142 acc_train: 0.7786 loss_val: 1.3003 acc_val: 0.7103 time: 5.3174s\n",
      "Epoch: 0068 loss_train: 1.1478 acc_train: 0.7143 loss_val: 1.2911 acc_val: 0.7118 time: 5.3989s\n",
      "Epoch: 0069 loss_train: 1.1768 acc_train: 0.7357 loss_val: 1.2823 acc_val: 0.7153 time: 5.2564s\n",
      "Epoch: 0070 loss_train: 1.1888 acc_train: 0.7929 loss_val: 1.2738 acc_val: 0.7188 time: 5.4028s\n",
      "Epoch: 0071 loss_train: 1.0819 acc_train: 0.7571 loss_val: 1.2651 acc_val: 0.7208 time: 5.3602s\n",
      "Epoch: 0072 loss_train: 1.1048 acc_train: 0.7500 loss_val: 1.2565 acc_val: 0.7227 time: 5.4233s\n",
      "Epoch: 0073 loss_train: 1.1959 acc_train: 0.7571 loss_val: 1.2480 acc_val: 0.7290 time: 5.2349s\n",
      "Epoch: 0074 loss_train: 1.0640 acc_train: 0.8000 loss_val: 1.2393 acc_val: 0.7301 time: 5.2385s\n",
      "Epoch: 0075 loss_train: 1.1433 acc_train: 0.7857 loss_val: 1.2309 acc_val: 0.7336 time: 5.2490s\n",
      "Epoch: 0076 loss_train: 1.1576 acc_train: 0.7857 loss_val: 1.2226 acc_val: 0.7348 time: 5.2397s\n",
      "Epoch: 0077 loss_train: 1.0936 acc_train: 0.6929 loss_val: 1.2142 acc_val: 0.7356 time: 5.2819s\n",
      "Epoch: 0078 loss_train: 1.0785 acc_train: 0.7786 loss_val: 1.2056 acc_val: 0.7379 time: 5.2221s\n",
      "Epoch: 0079 loss_train: 1.1187 acc_train: 0.7500 loss_val: 1.1975 acc_val: 0.7403 time: 5.2500s\n",
      "Epoch: 0080 loss_train: 0.9357 acc_train: 0.7857 loss_val: 1.1893 acc_val: 0.7410 time: 5.2349s\n",
      "Epoch: 0081 loss_train: 1.0518 acc_train: 0.7786 loss_val: 1.1811 acc_val: 0.7418 time: 5.2478s\n",
      "Epoch: 0082 loss_train: 1.0965 acc_train: 0.7214 loss_val: 1.1731 acc_val: 0.7430 time: 5.2615s\n",
      "Epoch: 0083 loss_train: 0.9630 acc_train: 0.7929 loss_val: 1.1653 acc_val: 0.7445 time: 5.3932s\n",
      "Epoch: 0084 loss_train: 0.9476 acc_train: 0.8143 loss_val: 1.1573 acc_val: 0.7453 time: 5.3281s\n",
      "Epoch: 0085 loss_train: 1.0241 acc_train: 0.7643 loss_val: 1.1495 acc_val: 0.7477 time: 5.3331s\n",
      "Epoch: 0086 loss_train: 1.0354 acc_train: 0.7571 loss_val: 1.1418 acc_val: 0.7496 time: 5.3351s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.9485 acc_train: 0.7857 loss_val: 1.1341 acc_val: 0.7512 time: 5.3250s\n",
      "Epoch: 0088 loss_train: 1.0428 acc_train: 0.7571 loss_val: 1.1265 acc_val: 0.7519 time: 5.2746s\n",
      "Epoch: 0089 loss_train: 1.0316 acc_train: 0.7857 loss_val: 1.1189 acc_val: 0.7523 time: 5.3293s\n",
      "Epoch: 0090 loss_train: 1.0198 acc_train: 0.7357 loss_val: 1.1112 acc_val: 0.7535 time: 5.2536s\n",
      "Epoch: 0091 loss_train: 1.0297 acc_train: 0.7643 loss_val: 1.1037 acc_val: 0.7547 time: 5.2376s\n",
      "Epoch: 0092 loss_train: 0.9423 acc_train: 0.8286 loss_val: 1.0962 acc_val: 0.7562 time: 5.2999s\n",
      "Epoch: 0093 loss_train: 0.9041 acc_train: 0.7714 loss_val: 1.0887 acc_val: 0.7566 time: 5.2304s\n",
      "Epoch: 0094 loss_train: 1.0070 acc_train: 0.8000 loss_val: 1.0815 acc_val: 0.7574 time: 5.2487s\n",
      "Epoch: 0095 loss_train: 0.9321 acc_train: 0.7714 loss_val: 1.0746 acc_val: 0.7582 time: 5.2711s\n",
      "Epoch: 0096 loss_train: 0.9708 acc_train: 0.7929 loss_val: 1.0680 acc_val: 0.7593 time: 5.2587s\n",
      "Epoch: 0097 loss_train: 0.8880 acc_train: 0.8000 loss_val: 1.0616 acc_val: 0.7613 time: 5.2485s\n",
      "Epoch: 0098 loss_train: 0.9247 acc_train: 0.7929 loss_val: 1.0553 acc_val: 0.7621 time: 5.3682s\n",
      "Epoch: 0099 loss_train: 0.8799 acc_train: 0.8143 loss_val: 1.0490 acc_val: 0.7632 time: 5.3778s\n",
      "Epoch: 0100 loss_train: 0.8528 acc_train: 0.7857 loss_val: 1.0427 acc_val: 0.7648 time: 5.3381s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 532.2965s\n",
      "Test set results: loss= 1.0427 accuracy= 0.7648\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6Ox3fbTG7rc"
   },
   "source": [
    "# Question: (Your task)\n",
    "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urJ8Q-neDzHU"
   },
   "source": [
    "<span style=\"color:green\">Answer:</span> GCN has higher training accuracy than GAT whereas, GAT has higher validation accuracy than the GCN. This result is validated by the test results for which GAT yielded higher accuracy and lower loss. As a result in terms of generalization performance GAT has performed better than GCN. I think GAT performed better because it leverages the attention mechanism to figure out which nodes to pay attention to and how much attention to pay. Additionally, it implements multi-head attention mechanism which allows it to control the mixing of information and in turn lead to better representations which is crucial for deep learning practices. Also, multi-head attention is parallelizable which is efficient. On the other hand, GAT training took longer than the GCN training. I think this is due to GAT having to perform more operations. With GAT, we need to do more work such as computing the attention scores for the nodes, and having multiple heads. So overall, I think that GCN is much easier to implement and quicker to train whereas, GAT performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esv21BaOtpOU"
   },
   "source": [
    "# Late Policy\n",
    "You may use up to 7 grace days over the course of the semester for the |practicals you will take. You will only use up to 3 grace days per assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eiab3C-Ztxg_"
   },
   "source": [
    "# Academic Integrity\n",
    "All work on assignments must be done individually unless stated otherwise. Turning in someone else’s work, in whole or in part, as your own will be considered as a violation of academic integrity. Please note that the former condition also holds for the material found on the web as everything on the web has been written by someone else.\n",
    "\n",
    "## Acknowledgements\n",
    "Adapted from University of Toronto, Neural Networks and Deep Learning course (CSC413/2516)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
